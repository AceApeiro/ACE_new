<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>ACE ANI XML Extractor (Single File)</title>
  <style>
    :root { --bg:#0b0f14; --panel:#101826; --panel2:#0f1622; --txt:#e7eef9; --muted:#91a4c7; --ok:#2ecc71; --warn:#f1c40f; --bad:#e74c3c; --btn:#1f6feb; --border:#21324a; }
    body { margin:0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial; background:var(--bg); color:var(--txt); }
    header { padding:14px 16px; border-bottom:1px solid var(--border); display:flex; gap:12px; align-items:center; justify-content:space-between; background:linear-gradient(180deg,#0b0f14,#0a0e13); position:sticky; top:0; z-index:10;}
    header h1 { font-size:14px; margin:0; letter-spacing:0.4px; color:var(--txt); }
    .wrap { display:grid; grid-template-columns: 360px 1fr 420px; gap:12px; padding:12px; }
    .card { background:var(--panel); border:1px solid var(--border); border-radius:14px; padding:12px; box-shadow:0 8px 28px rgba(0,0,0,.25); }
    .card h2 { margin:0 0 10px; font-size:13px; color:var(--muted); letter-spacing:.3px; }
    label { display:block; font-size:12px; color:var(--muted); margin:10px 0 6px; }
    input[type="file"], textarea { width:100%; box-sizing:border-box; background:var(--panel2); border:1px solid var(--border); color:var(--txt); border-radius:12px; padding:10px; }
    textarea { min-height:120px; resize:vertical; font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; font-size:12px; line-height:1.4; }
    .row { display:flex; gap:10px; flex-wrap:wrap; }
    button { background:var(--btn); color:white; border:none; padding:10px 12px; border-radius:12px; cursor:pointer; font-weight:600; font-size:12px; }
    button.secondary { background:#2b3648; }
    button.danger { background:#b42318; }
    button:disabled { opacity:.55; cursor:not-allowed; }
    .status { font-size:12px; padding:10px; border-radius:12px; border:1px solid var(--border); background:var(--panel2); line-height:1.4; white-space:pre-wrap; }
    .tag { display:inline-block; padding:3px 8px; border-radius:999px; font-size:11px; margin-right:6px; border:1px solid var(--border); background:#0d1420; color:var(--muted); }
    .tag.ok { color:#b7ffd0; border-color:rgba(46,204,113,.35); background:rgba(46,204,113,.08); }
    .tag.warn { color:#fff3b0; border-color:rgba(241,196,15,.35); background:rgba(241,196,15,.08); }
    .tag.bad { color:#ffd1cc; border-color:rgba(231,76,60,.35); background:rgba(231,76,60,.08); }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; font-size:12px; }
    iframe { width:100%; height:72vh; border:1px solid var(--border); border-radius:14px; background:white; }
    .small { font-size:11px; color:var(--muted); margin-top:6px; }
    .divider { height:1px; background:var(--border); margin:10px 0; }
    .kv { display:grid; grid-template-columns:120px 1fr; gap:6px; font-size:12px; }
    .kv div:nth-child(odd){ color:var(--muted); }
  </style>

  <!-- PDF.js (CDN) -->
  <script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@4.10.38/build/pdf.min.js"></script>
</head>

<body>
<header>
  <h1>ACE — ANI XML Extractor (Single File)</h1>
  <div class="row">
    <button id="btnValidate" class="secondary">1) Validate (ID/Title Rules)</button>
    <button id="btnExtract">2) Extract + Build XML</button>
    <button id="btnDownload" disabled>3) Download XML</button>
    <button id="btnReset" class="danger">Reset</button>
  </div>
</header>

<div class="wrap">
  <!-- LEFT: Inputs -->
  <section class="card">
    <h2>Support Documents</h2>

    <label>PDF (required)</label>
    <input id="pdfFile" type="file" accept="application/pdf" />

    <label>HTML Summary (required)</label>
    <input id="htmlFile" type="file" accept=".html,text/html" />

    <label>Scraping Data Source (paste OR upload .txt/.json)</label>
    <input id="scrapeFile" type="file" accept=".txt,.json,text/plain,application/json" />
    <label>Or paste scraping text/json here</label>
    <textarea id="scrapeText" placeholder="Paste scraping text / JSON here..."></textarea>

    <div class="divider"></div>

    <h2>Rules Applied (Strict)</h2>
    <div class="status">
- If PDF has arXiv ID:
  ✅ require PDF ID+ver == HTML (Cite as SECOND line) == Scrape ID+ver
  ❌ else HOLD: Version mismatched

- If PDF has NO arXiv ID:
  1) compare titles PDF vs HTML
     - if mismatch in content -> HOLD
  2) require HTML ID+ver == Scrape ID+ver
     - else HOLD
    </div>

    <div class="small">
      HTML rule: **Cite as → SECOND NON-EMPTY LINE AFTER it** (no fallback to first line).
    </div>
  </section>

  <!-- CENTER: PDF Preview -->
  <section class="card">
    <h2>PDF Preview</h2>
    <iframe id="pdfPreview" title="PDF Preview"></iframe>
    <div class="small mono" id="pdfMeta"></div>
  </section>

  <!-- RIGHT: Output -->
  <section class="card">
    <h2>Validation + Extracted Fields</h2>

    <div id="badges"></div>
    <div class="divider"></div>

    <div class="kv">
      <div>PDF arXiv</div><div class="mono" id="outPdfId">—</div>
      <div>HTML arXiv</div><div class="mono" id="outHtmlId">—</div>
      <div>Scrape arXiv</div><div class="mono" id="outScrapeId">—</div>
      <div>Status</div><div class="mono" id="outStatus">—</div>
    </div>

    <div class="divider"></div>

    <label>Title (final rule applied)</label>
    <textarea id="outTitle" class="mono" placeholder="Title will appear here..."></textarea>

    <label>Keywords</label>
    <textarea id="outKeywords" class="mono" placeholder="One per line..."></textarea>

    <label>Abstract</label>
    <textarea id="outAbstract" class="mono" placeholder="Abstract text..."></textarea>

    <label>Authors + Affiliations (parsed)</label>
    <textarea id="outAuthors" class="mono" placeholder="Author blocks..."></textarea>

    <label>References (parsed)</label>
    <textarea id="outRefs" class="mono" placeholder="Reference list..."></textarea>

    <div class="divider"></div>
    <label>Generated ANI XML</label>
    <textarea id="outXml" class="mono" placeholder="XML output will appear here..."></textarea>
  </section>
</div>

<script>
/* =========================
   Helpers (safe + strict)
========================= */

function escXml(s){
  return (s ?? "")
    .replace(/&/g,"&amp;")
    .replace(/</g,"&lt;")
    .replace(/>/g,"&gt;")
    .replace(/"/g,"&quot;")
    .replace(/'/g,"&apos;");
}

function nowIso(){
  return new Date().toISOString();
}

// Parse arXiv id+version from text
// Supports: "arXiv:2409.09977v2", "2409.09977v2", and ignores bracket junk
function parseArxivIdVersion(s){
  const t = (s||"").replace(/\u00A0/g," ").trim();
  if (!t) return null;

  // normalize common noise
  const clean = t.replace(/\(([^)]*)\)/g," ").replace(/\[([^\]]*)\]/g," ").trim();

  // Prefer explicit arXiv:
  let m = clean.match(/arxiv\s*:\s*(\d{4}\.\d{4,5})(v\d+)?/i);
  if (m) return { id: m[1], version: (m[2]||"").toLowerCase(), full: m[1]+(m[2]||"") };

  // Or plain id
  m = clean.match(/(\d{4}\.\d{4,5})(v\d+)?/i);
  if (m) return { id: m[1], version: (m[2]||"").toLowerCase(), full: m[1]+(m[2]||"") };

  return null;
}

// STRICT compare
function sameArxiv(a,b){
  if (!a || !b) return false;
  const av = (a.version||"").toLowerCase();
  const bv = (b.version||"").toLowerCase();
  return a.id === b.id && av === bv;
}

function normSpace(s){
  return (s||"").replace(/\s+/g," ").trim();
}

// Title compare (your rules):
// - If totally different -> choose HTML
// - If not totally different -> choose the more complete (longer) title
// - If any punctuation/structure difference like extra hyphen you said to treat as difference → choose HTML
// (We implement: if normalized strings differ by any non-trivial punctuation change → HTML wins)
function chooseTitle(pdfTitle, htmlTitle){
  const p = normSpace(pdfTitle);
  const h = normSpace(htmlTitle);
  if (!p && h) return h;
  if (!h && p) return p;
  if (!p && !h) return "";

  // If exact match => keep either
  if (p === h) return p;

  // If punctuation differs (like extra hyphen) => always use HTML (your instruction)
  // We detect punctuation differences by stripping letters/numbers/spaces and comparing the remaining punctuation patterns.
  const pPunct = p.replace(/[A-Za-z0-9\s]/g,"");
  const hPunct = h.replace(/[A-Za-z0-9\s]/g,"");
  if (pPunct !== hPunct) return h;

  // Content difference check (token overlap)
  const pTokens = new Set(p.toLowerCase().split(/\s+/).filter(Boolean));
  const hTokens = new Set(h.toLowerCase().split(/\s+/).filter(Boolean));
  const inter = [...pTokens].filter(x=>hTokens.has(x)).length;
  const minSize = Math.min(pTokens.size, hTokens.size) || 1;
  const overlap = inter / minSize;

  // "Totally different" heuristic
  if (overlap < 0.55) return h;

  // Not totally different: choose more complete (longer)
  return (h.length > p.length) ? h : p;
}

// Extract keywords from PDF text (basic heading based)
function extractKeywordsFromPdfText(pdfText){
  const text = pdfText || "";
  const headings = [
    "Keywords", "Index Terms", "Keywords and Phrases", "Subject Headings",
    "Key words", "KEYWORDS", "INDEX TERMS"
  ];

  // Find first occurrence of a heading line
  // We'll split into lines and search for lines that start with heading
  const lines = text.split("\n").map(l=>l.trim()).filter(Boolean);

  let idx = -1;
  let head = "";
  for (let i=0;i<lines.length;i++){
    const l = lines[i];
    for (const h of headings){
      const re = new RegExp("^" + h.replace(/[.*+?^${}()|[\]\\]/g,"\\$&") + "\\b\\s*[:\\-–—]?", "i");
      if (re.test(l)){
        idx = i; head = h; break;
      }
    }
    if (idx !== -1) break;
  }

  if (idx === -1) return [];

  // Collect a small window after heading until blank-ish stop (or Abstract/Introduction)
  const stopRe = /^(abstract|introduction|1\s+introduction)\b/i;
  let chunk = lines[idx];

  // remove heading itself
  chunk = chunk.replace(new RegExp("^" + head + "\\b\\s*[:\\-–—]?", "i"), "").trim();

  let j = idx+1;
  while (j < lines.length && j < idx+10){
    const l = lines[j];
    if (stopRe.test(l)) break;
    // stop if new section number starts strongly
    if (/^\d+(\.\d+)?\s+\S+/.test(l) && l.length < 60) break;
    chunk += " " + l;
    j++;
  }

  // Split by separators (NOT spaces)
  const parts = chunk
    .replace(/\s+/g," ")
    .split(/\s*(;|•|·|\||,|–|—|-|\u2022)\s*/g)
    .filter(x => x && !/^(;|•|·|\||,|–|—|-|\u2022)$/.test(x))
    .map(x => x.trim())
    .filter(Boolean);

  // De-duplicate, remove the word "Keyword(s)" if embedded
  const out = [];
  const seen = new Set();
  for (let k of parts){
    k = k.replace(/^keywords?\b[:\-–—]?\s*/i,"").trim();
    if (!k) continue;
    const key = k.toLowerCase();
    if (seen.has(key)) continue;
    seen.add(key);
    out.push(k);
  }
  return out;
}

// Extract abstract from PDF text (basic, keeps content)
function extractAbstractFromPdfText(pdfText){
  const text = pdfText || "";
  const lines = text.split("\n").map(l=>l.trim());

  // Find a line that is exactly "Abstract" or begins with it
  let idx = lines.findIndex(l => /^(abstract|summary|synopsis|summary and conclusions)\b/i.test(l));
  if (idx === -1) return "";

  // If the heading line also has text after it, include it
  let chunk = lines[idx].replace(/^(abstract|summary|synopsis|summary and conclusions)\b\s*[:\-–—]?\s*/i,"").trim();

  // collect until we hit a likely next section
  for (let i=idx+1; i<lines.length && i<idx+80; i++){
    const l = lines[i];
    if (!l) continue;
    if (/^(keywords|index terms|1\s+introduction|introduction)\b/i.test(l)) break;
    if (/^\d+(\.\d+)?\s+\S+/.test(l) && l.length < 60) break;
    chunk += (chunk ? " " : "") + l;
  }

  // normalize spaces
  return normSpace(chunk);
}

// Very rough reference extraction (find "References" then split)
function extractReferencesFromPdfText(pdfText){
  const text = pdfText || "";
  const lower = text.toLowerCase();
  const pos = lower.lastIndexOf("\nreferences");
  if (pos === -1) return [];

  const tail = text.slice(pos);
  // remove heading
  const tail2 = tail.replace(/^\s*references\s*/i,"").trim();

  // split by patterns: [1], 1., 1 , etc.
  const rawLines = tail2.split("\n").map(x=>x.trim()).filter(Boolean);

  // merge lines into reference blocks
  const refs = [];
  let cur = "";
  for (const l of rawLines){
    const startsNew = /^(\[\d+\]|\d+\.)\s+/.test(l) || /^[A-Z][a-zA-Z]+,\s/.test(l);
    if (startsNew && cur){
      refs.push(normSpace(cur));
      cur = l;
    } else {
      cur += (cur ? " " : "") + l;
    }
  }
  if (cur) refs.push(normSpace(cur));

  // trim to sane size
  return refs.slice(0, 400);
}

// Author + affiliation parsing:
// We try to find emails and numbered affiliations.
function parseAuthorsAffilsFromPdfText(pdfText){
  const text = (pdfText||"").replace(/\u00A0/g," ");
  const lines = text.split("\n").map(l=>l.trim()).filter(Boolean);

  // emails
  const emails = [];
  const emailRe = /[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}/ig;
  for (const l of lines){
    const ms = l.match(emailRe);
    if (ms) ms.forEach(e=>emails.push(e));
  }
  const uniqEmails = [...new Set(emails.map(e=>e.toLowerCase()))];

  // numbered affiliations: "1 Dept ...", "2 School ...", etc.
  const affMap = new Map(); // num -> text
  for (const l of lines){
    const m = l.match(/^(\d{1,2})\s*(.+)$/);
    if (m && m[2] && m[2].length > 8){
      // ignore if it looks like section numbering
      if (/^(introduction|abstract|references)\b/i.test(m[2])) continue;
      affMap.set(m[1], m[2].trim());
    }
  }

  // guess author line near top: first 40 lines until Abstract
  const top = [];
  for (let i=0;i<Math.min(lines.length, 120); i++){
    if (/^abstract\b/i.test(lines[i])) break;
    top.push(lines[i]);
  }
  const topText = top.join(" ");

  // heuristic: authors often appear before affiliations and emails
  // take the longest comma-separated name-ish segment
  // We'll attempt to locate a segment before first affiliation number block or first email.
  let cut = topText;
  const emailPos = uniqEmails.length ? topText.toLowerCase().indexOf(uniqEmails[0]) : -1;
  if (emailPos > 0) cut = topText.slice(0, emailPos);

  // Extract possible names: sequences of capitalized words with optional initials
  const nameCand = cut.match(/([A-Z][a-zA-Z\-’']+(?:\s+[A-Z]\.)?(?:\s+[A-Z][a-zA-Z\-’']+){0,3})/g) || [];
  // Filter out common words
  const bad = new Set(["Abstract","Introduction","Preprint","Finding","Off","Grid","Proceedings","University","Department","School"]);
  const filtered = nameCand
    .map(x=>x.trim())
    .filter(x=>x.length>=3 && !bad.has(x))
    .slice(0, 30);

  // dedupe
  const names = [];
  const seen = new Set();
  for (const n of filtered){
    const key = n.toLowerCase();
    if (seen.has(key)) continue;
    seen.add(key);
    names.push(n);
  }

  // If we cannot confidently parse, return empty names but still emails/affs
  return { names, affMap, emails: uniqEmails };
}

/* =========================
   HTML parsing (STRICT!)
========================= */

// HTML rule (STRICT): find "Cite as" then use the SECOND NON-EMPTY LINE AFTER IT.
// We intentionally IGNORE the same line as "Cite as" and the first line after it.
function extractArxivFromHtmlCiteAs(htmlText){
  const raw = (htmlText || "");
  if (!raw) return null;

  // Convert HTML to plain text lines (handles tags / tables / layout)
  let text = raw;
  try {
    const doc = new DOMParser().parseFromString(raw, "text/html");
    text = (doc.body?.innerText || doc.body?.textContent || raw);
  } catch {
    text = raw.replace(/<script[\s\S]*?<\/script>/gi, " ")
              .replace(/<style[\s\S]*?<\/style>/gi, " ")
              .replace(/<[^>]+>/g, " ");
  }

  const lines = text.replace(/\r/g, "")
    .split("\n")
    .map(l => l.trim())
    .filter(Boolean);

  const citeIdx = lines.findIndex(l => /(^|\b)cite\s+as\b/i.test(l));
  if (citeIdx === -1) return null;

  // STRICT: second non-empty line AFTER cite
  const secondLine = lines[citeIdx + 2] || "";
  const parsed = parseArxivIdVersion(secondLine);
  if (parsed && parsed.id) return parsed;

  // Safer fallback: within next lines, pick SECOND arxiv-like line only
  const after = lines.slice(citeIdx + 1, citeIdx + 20);
  const arxivLines = after.filter(l => /(\d{4}\.\d{4,5})(v\d+)?/i.test(l) || /arxiv\s*:/i.test(l));
  if (arxivLines.length >= 2) {
    const parsed2 = parseArxivIdVersion(arxivLines[1]); // STRICT second match
    if (parsed2 && parsed2.id) return parsed2;
  }

  return null; // no fallback to first line
}

function extractTitleFromHtml(htmlText){
  const raw = htmlText || "";
  if (!raw) return "";

  try {
    const doc = new DOMParser().parseFromString(raw, "text/html");
    // arXiv HTML often has h1.title or meta tags
    const h1 = doc.querySelector("h1.title, h1, .title");
    if (h1 && h1.textContent) return normSpace(h1.textContent.replace(/^title\s*:\s*/i,""));
    const meta = doc.querySelector('meta[name="citation_title"], meta[property="og:title"]');
    if (meta?.getAttribute("content")) return normSpace(meta.getAttribute("content"));
    // fallback: first meaningful title tag
    if (doc.title) return normSpace(doc.title);
  } catch {}

  return "";
}

/* =========================
   Scrape parsing
========================= */

function extractArxivFromScrape(scrapeText){
  const raw = (scrapeText||"").trim();
  if (!raw) return null;

  // If JSON, try find id fields
  if (raw.startsWith("{") || raw.startsWith("[")) {
    try {
      const obj = JSON.parse(raw);
      const flat = JSON.stringify(obj);
      const parsed = parseArxivIdVersion(flat);
      if (parsed) return parsed;
    } catch {}
  }

  // Otherwise parse from text
  // You said brackets can be ignored, focus on v#
  // Use parseArxivIdVersion directly
  return parseArxivIdVersion(raw);
}

/* =========================
   PDF parsing via PDF.js
========================= */

pdfjsLib.GlobalWorkerOptions.workerSrc =
  "https://cdn.jsdelivr.net/npm/pdfjs-dist@4.10.38/build/pdf.worker.min.js";

async function readFileAsText(file){
  return new Promise((resolve,reject)=>{
    const r = new FileReader();
    r.onload = () => resolve(String(r.result||""));
    r.onerror = reject;
    r.readAsText(file);
  });
}

async function readFileAsArrayBuffer(file){
  return new Promise((resolve,reject)=>{
    const r = new FileReader();
    r.onload = () => resolve(r.result);
    r.onerror = reject;
    r.readAsArrayBuffer(file);
  });
}

async function extractPdfText(pdfArrayBuffer, maxPages = 6, alsoLastPages = 2){
  const doc = await pdfjsLib.getDocument({data: pdfArrayBuffer}).promise;
  const total = doc.numPages;

  const pagesToRead = new Set();
  for (let i=1;i<=Math.min(total, maxPages);i++) pagesToRead.add(i);
  for (let i=Math.max(1, total - alsoLastPages + 1); i<=total; i++) pagesToRead.add(i);

  const ordered = [...pagesToRead].sort((a,b)=>a-b);

  let out = "";
  for (const pno of ordered){
    const page = await doc.getPage(pno);
    const content = await page.getTextContent();
    const strs = content.items.map(it => it.str || "");
    out += "\n" + strs.join(" ") + "\n";
  }
  return { text: out, numPages: total };
}

function extractArxivFromPdfText(pdfText){
  // Look for arXiv:####.#####v#
  const m = (pdfText||"").match(/arxiv\s*:\s*(\d{4}\.\d{4,5})(v\d+)?/i);
  if (m) return { id:m[1], version:(m[2]||"").toLowerCase(), full:m[1]+(m[2]||"") };

  // Sometimes footer has "####.#####v#"
  const n = (pdfText||"").match(/(\d{4}\.\d{4,5})(v\d+)\b/i);
  if (n) return { id:n[1], version:(n[2]||"").toLowerCase(), full:n[1]+n[2] };

  return null;
}

// Simple PDF title heuristic: try first line with many words near top
function extractTitleFromPdfText(pdfText){
  const lines = (pdfText||"").split("\n").map(l=>l.trim()).filter(Boolean);
  // avoid junk
  const bad = /^(arxiv|preprint|abstract|keywords|index terms|the|a|an)\b/i;

  // pick the first long-ish line that isn't a heading, not too long
  for (const l of lines.slice(0, 80)){
    if (l.length < 14) continue;
    if (l.length > 220) continue;
    if (bad.test(l)) continue;
    // must have at least 4 words
    if (l.split(/\s+/).length < 4) continue;
    return normSpace(l.replace(/\s*\*+\s*$/,"")); // remove trailing *
  }
  return "";
}

/* =========================
   ANI XML builder
========================= */

function buildAniXml(payload){
  const {
    arxivFull, title, keywords, abstractText,
    authors, affiliations, refs
  } = payload;

  const ts = nowIso();

  // Build author-groups:
  // authors: [{ initials, given, surname, email, affNums:[] }]
  // affiliations: map num->text
  let authorGroupsXml = "";
  let groupSeq = 1;

  for (let i=0;i<authors.length;i++){
    const a = authors[i];
    const seq = i+1;

    const affTextParts = [];
    if (a.affNums && a.affNums.length){
      for (const n of a.affNums){
        if (affiliations[n]) affTextParts.push(`${n} ${affiliations[n]}`);
      }
    }

    // If no numbered aff, just dump all affiliations as source-text joined
    let sourceText = affTextParts.length
      ? affTextParts.join(" ")
      : Object.entries(affiliations).map(([n,v]) => `${n} ${v}`).join(" ");

    sourceText = normSpace(sourceText);

    authorGroupsXml += `
<author-group seq="${groupSeq}">
  <author seq="${seq}">
    ${a.initials ? `<ce:initials>${escXml(a.initials)}</ce:initials>` : ``}
    ${a.surname ? `<ce:surname>${escXml(a.surname)}</ce:surname>` : ``}
    ${a.given ? `<ce:given-name>${escXml(a.given)}</ce:given-name>` : ``}
    ${a.email ? `<ce:e-address>${escXml(a.email)}</ce:e-address>` : ``}
  </author>
  <affiliation>
    <ce:source-text>${escXml(sourceText)}</ce:source-text>
  </affiliation>
</author-group>`.trim() + "\n";

    groupSeq++;
  }

  // Keywords
  let kwXml = "";
  if (keywords && keywords.length){
    kwXml = `
<author-keywords>
${keywords.map(k=>`  <author-keyword>${escXml(k)}</author-keyword>`).join("\n")}
</author-keywords>`.trim();
  }

  // References
  const refItems = (refs||[]).filter(Boolean);
  const bibXml = `
<bibliography refcount="${refItems.length}">
${refItems.map((r,i)=>`
  <reference seq="${i+1}">
    <ref-info/>
    <ref-fulltext>${escXml(r)}</ref-fulltext>
    <ce:source-text>${escXml(r)}</ce:source-text>
  </reference>`.trim()).join("\n")}
</bibliography>`.trim();

  const xml =
`<?xml version="1.0" encoding="UTF-8"?>
<units xmlns="http://www.elsevier.com/xml/ani/ani" xmlns:ce="http://www.elsevier.com/xml/ani/common">
  <unit type="ARTICLE">
    <unit-info>
      <unit-id>1</unit-id>
      <order-id>unknown</order-id>
      <parcel-id>none</parcel-id>
      <supplier-id>4</supplier-id>
      <timestamp>${escXml(ts)}</timestamp>
    </unit-info>
    <unit-content>
      <bibrecord>
        <item-info>
          <status state="new"/>
          <itemidlist>
            <itemid idtype="ARXIV">${escXml(arxivFull || "")}</itemid>
          </itemidlist>
        </item-info>
        <head>
          <citation-info>
            <citation-type code="ar"/>
            <citation-language xml:lang="ENG"/>
            <abstract-language xml:lang="ENG"/>
            ${kwXml ? kwXml : ``}
          </citation-info>
          <citation-title>
            <titletext xml:lang="ENG" original="y">${escXml(title || "")}</titletext>
          </citation-title>
          ${authorGroupsXml}
          <abstracts>
            <abstract original="y" xml:lang="ENG">
              <ce:para>${escXml(abstractText || "")}</ce:para>
            </abstract>
          </abstracts>
          <source srcid="???"/>
        </head>
        <tail>
          ${bibXml}
        </tail>
      </bibrecord>
    </unit-content>
  </unit>
</units>`;

  return xml;
}

/* =========================
   App state + UI
========================= */

const els = {
  pdfFile: document.getElementById("pdfFile"),
  htmlFile: document.getElementById("htmlFile"),
  scrapeFile: document.getElementById("scrapeFile"),
  scrapeText: document.getElementById("scrapeText"),
  pdfPreview: document.getElementById("pdfPreview"),
  pdfMeta: document.getElementById("pdfMeta"),

  btnValidate: document.getElementById("btnValidate"),
  btnExtract: document.getElementById("btnExtract"),
  btnDownload: document.getElementById("btnDownload"),
  btnReset: document.getElementById("btnReset"),

  badges: document.getElementById("badges"),
  outPdfId: document.getElementById("outPdfId"),
  outHtmlId: document.getElementById("outHtmlId"),
  outScrapeId: document.getElementById("outScrapeId"),
  outStatus: document.getElementById("outStatus"),
  outTitle: document.getElementById("outTitle"),
  outKeywords: document.getElementById("outKeywords"),
  outAbstract: document.getElementById("outAbstract"),
  outAuthors: document.getElementById("outAuthors"),
  outRefs: document.getElementById("outRefs"),
  outXml: document.getElementById("outXml"),
};

let STATE = {
  pdfBuf: null,
  pdfText: "",
  pdfPages: 0,

  htmlText: "",
  scrapeRaw: "",

  pdfArxiv: null,
  htmlArxiv: null,
  scrapeArxiv: null,

  pdfTitle: "",
  htmlTitle: "",
  finalTitle: "",

  validated: false,
  holdReason: "",

  xml: ""
};

function badge(text, cls){
  const span = document.createElement("span");
  span.className = "tag " + cls;
  span.textContent = text;
  return span;
}

function setBadges(items){
  els.badges.innerHTML = "";
  items.forEach(it => els.badges.appendChild(badge(it.text, it.cls)));
}

function setStatusLine(msg){
  els.outStatus.textContent = msg || "—";
}

function resetAll(){
  STATE = {
    pdfBuf:null, pdfText:"", pdfPages:0,
    htmlText:"", scrapeRaw:"",
    pdfArxiv:null, htmlArxiv:null, scrapeArxiv:null,
    pdfTitle:"", htmlTitle:"", finalTitle:"",
    validated:false, holdReason:"",
    xml:""
  };

  els.pdfFile.value = "";
  els.htmlFile.value = "";
  els.scrapeFile.value = "";
  els.scrapeText.value = "";

  els.pdfPreview.src = "about:blank";
  els.pdfMeta.textContent = "";

  els.outPdfId.textContent = "—";
  els.outHtmlId.textContent = "—";
  els.outScrapeId.textContent = "—";
  setStatusLine("—");

  els.outTitle.value = "";
  els.outKeywords.value = "";
  els.outAbstract.value = "";
  els.outAuthors.value = "";
  els.outRefs.value = "";
  els.outXml.value = "";

  els.btnDownload.disabled = true;
  setBadges([]);
}

els.btnReset.addEventListener("click", resetAll);

els.pdfFile.addEventListener("change", async () => {
  const f = els.pdfFile.files?.[0];
  if (!f) return;

  const ab = await readFileAsArrayBuffer(f);
  STATE.pdfBuf = ab;

  // preview
  const url = URL.createObjectURL(f);
  els.pdfPreview.src = url;

  // extract text (first 6 + last 2 pages)
  const {text, numPages} = await extractPdfText(ab, 6, 2);
  STATE.pdfText = text;
  STATE.pdfPages = numPages;

  // parse arxiv + title
  STATE.pdfArxiv = extractArxivFromPdfText(text);
  STATE.pdfTitle = extractTitleFromPdfText(text);

  els.pdfMeta.textContent = `Pages: ${numPages} | PDF title guess: ${STATE.pdfTitle || "—"}`;

  updateOutputsQuick();
});

els.htmlFile.addEventListener("change", async () => {
  const f = els.htmlFile.files?.[0];
  if (!f) return;
  const t = await readFileAsText(f);
  STATE.htmlText = t;

  STATE.htmlArxiv = extractArxivFromHtmlCiteAs(t); // STRICT cite-as second line
  STATE.htmlTitle = extractTitleFromHtml(t);

  updateOutputsQuick();
});

els.scrapeFile.addEventListener("change", async () => {
  const f = els.scrapeFile.files?.[0];
  if (!f) return;
  const t = await readFileAsText(f);
  STATE.scrapeRaw = t;
  STATE.scrapeArxiv = extractArxivFromScrape(t);

  updateOutputsQuick();
});

els.scrapeText.addEventListener("input", () => {
  STATE.scrapeRaw = els.scrapeText.value || "";
  STATE.scrapeArxiv = extractArxivFromScrape(STATE.scrapeRaw);
  updateOutputsQuick();
});

function updateOutputsQuick(){
  els.outPdfId.textContent = STATE.pdfArxiv?.full || "—";
  els.outHtmlId.textContent = STATE.htmlArxiv?.full || "—";
  els.outScrapeId.textContent = STATE.scrapeArxiv?.full || "—";

  // badges
  const items = [];
  if (STATE.pdfBuf) items.push({text:"PDF loaded", cls:"ok"});
  else items.push({text:"PDF missing", cls:"bad"});

  if (STATE.htmlText) items.push({text:"HTML loaded", cls:"ok"});
  else items.push({text:"HTML missing", cls:"bad"});

  if (STATE.scrapeRaw) items.push({text:"Scrape loaded", cls:"ok"});
  else items.push({text:"Scrape optional (but needed for gate)", cls:"warn"});

  if (STATE.htmlArxiv) items.push({text:"HTML arXiv from Cite as (2nd line)", cls:"ok"});
  else if (STATE.htmlText) items.push({text:"HTML arXiv not found via Cite as 2nd line", cls:"bad"});

  setBadges(items);
}

function hold(reason){
  STATE.validated = false;
  STATE.holdReason = reason || "HOLD";
  setStatusLine("HOLD — " + STATE.holdReason);
  setBadges([{text:"HOLD", cls:"bad"}]);
}

function ok(msg){
  STATE.validated = true;
  STATE.holdReason = "";
  setStatusLine(msg || "OK");
  setBadges([{text:"OK", cls:"ok"}]);
}

/* =========================
   Validation (your rules)
========================= */
els.btnValidate.addEventListener("click", async () => {
  // Must have PDF + HTML
  if (!STATE.pdfBuf) return hold("PDF missing");
  if (!STATE.htmlText) return hold("HTML missing");

  // Must have scraping source for the gate (you require html+scrape match)
  if (!STATE.scrapeRaw) return hold("Scraping source missing (paste or upload)");

  // Parse arxiv already computed
  const pdfA = STATE.pdfArxiv;
  const htmlA = STATE.htmlArxiv;  // STRICT
  const scrA = STATE.scrapeArxiv;

  // Title
  const pdfTitle = STATE.pdfTitle || "";
  const htmlTitle = STATE.htmlTitle || "";
  const finalTitle = chooseTitle(pdfTitle, htmlTitle);
  STATE.finalTitle = finalTitle;
  els.outTitle.value = finalTitle;

  // Gate logic
  if (pdfA && pdfA.id){
    // Need all three IDs and versions identical
    if (!htmlA) return hold("HTML arXiv not found (must use Cite as SECOND line)");
    if (!scrA) return hold("Scrape arXiv not found");

    if (!sameArxiv(pdfA, htmlA) || !sameArxiv(pdfA, scrA)){
      return hold(`Version mismatched (PDF=${pdfA.full} | HTML=${htmlA?.full||"—"} | Scrape=${scrA?.full||"—"})`);
    }
    ok("OK — IDs match across PDF/HTML/Scrape. Ready to process.");
    return;
  }

  // PDF has NO ID:
  // 1) Titles must match (content-wise). Your strict punctuation rule makes HTML chosen if punctuation differs.
  // We enforce: if titles are empty or too different -> HOLD.
  const p = normSpace(pdfTitle);
  const h = normSpace(htmlTitle);

  if (!p || !h) return hold("PDF title or HTML title missing (cannot use no-ID pathway)");
  // content overlap check
  const pTokens = new Set(p.toLowerCase().split(/\s+/).filter(Boolean));
  const hTokens = new Set(h.toLowerCase().split(/\s+/).filter(Boolean));
  const inter = [...pTokens].filter(x=>hTokens.has(x)).length;
  const overlap = inter / (Math.min(pTokens.size, hTokens.size) || 1);
  if (overlap < 0.55) return hold("PDF has no ID and titles are content-mismatched");

  // 2) HTML and Scrape arxiv must match
  if (!htmlA) return hold("HTML arXiv not found (must use Cite as SECOND line)");
  if (!scrA) return hold("Scrape arXiv not found");
  if (!sameArxiv(htmlA, scrA)) return hold(`Version mismatched (HTML=${htmlA.full} | Scrape=${scrA.full})`);

  ok("OK — PDF has no ID, titles match, and HTML/Scrape IDs match. Ready to process.");
});

/* =========================
   Extraction + XML build
========================= */
els.btnExtract.addEventListener("click", async () => {
  if (!STATE.pdfBuf) return hold("PDF missing");
  if (!STATE.htmlText) return hold("HTML missing");
  if (!STATE.scrapeRaw) return hold("Scraping source missing");
  if (!STATE.validated) return hold("Run Validate first (strict gate) and ensure OK");

  // Keywords + Abstract from PDF
  const kws = extractKeywordsFromPdfText(STATE.pdfText);
  els.outKeywords.value = kws.join("\n");

  const abs = extractAbstractFromPdfText(STATE.pdfText);
  els.outAbstract.value = abs;

  // Authors + affiliations
  const aa = parseAuthorsAffilsFromPdfText(STATE.pdfText);

  // Build author objects:
  // If we have no names, fallback to empty list (still valid XML but minimal)
  const authors = [];
  const emails = aa.emails || [];
  const affNums = [...aa.affMap.keys()].sort((a,b)=>Number(a)-Number(b));

  // Simple mapping: assign emails by order to names (best-effort)
  for (let i=0;i<aa.names.length;i++){
    const full = aa.names[i];
    const parts = full.split(/\s+/);
    const given = parts.slice(0, -1).join(" ");
    const surname = parts.slice(-1)[0] || "";
    const initials = (given ? given.split(/\s+/).map(w=>w[0]?.toUpperCase()+".").join("") : "");
    authors.push({
      given, surname, initials,
      email: emails[i] || "",
      affNums: affNums.length ? affNums : []
    });
  }

  // if no names parsed, create a single anonymous author if emails exist
  if (!authors.length && emails.length){
    authors.push({
      given:"", surname:"", initials:"",
      email: emails[0],
      affNums: affNums.length ? affNums : []
    });
  }

  // Show authors summary
  let authorDump = "";
  authorDump += `Names parsed: ${aa.names.length}\n`;
  authorDump += `Emails parsed: ${emails.length}\n\n`;
  authorDump += `Affiliations (numbered):\n`;
  for (const [n,v] of aa.affMap.entries()){
    authorDump += `${n} ${v}\n`;
  }
  authorDump += `\nAuthor records:\n`;
  for (const a of authors){
    authorDump += `- ${a.given} ${a.surname} | ${a.email} | aff: ${a.affNums.join(",")}\n`;
  }
  els.outAuthors.value = authorDump.trim();

  // References
  const refs = extractReferencesFromPdfText(STATE.pdfText);
  els.outRefs.value = refs.join("\n\n");

  // Determine arxivFull to output: prefer the matched arxiv from gate
  const arxivFull = (STATE.pdfArxiv?.full) || (STATE.htmlArxiv?.full) || (STATE.scrapeArxiv?.full) || "";

  // Build XML
  const xml = buildAniXml({
    arxivFull,
    title: STATE.finalTitle || chooseTitle(STATE.pdfTitle, STATE.htmlTitle),
    keywords: kws,
    abstractText: abs,
    authors,
    affiliations: Object.fromEntries(aa.affMap.entries()),
    refs
  });

  STATE.xml = xml;
  els.outXml.value = xml;
  els.btnDownload.disabled = false;

  ok("OK — Extraction complete + XML generated.");
});

els.btnDownload.addEventListener("click", () => {
  if (!STATE.xml) return;
  const blob = new Blob([STATE.xml], {type:"application/xml;charset=utf-8"});
  const a = document.createElement("a");
  a.href = URL.createObjectURL(blob);
  a.download = "output.xml";
  a.click();
  setTimeout(()=>URL.revokeObjectURL(a.href), 2000);
});

/* init */
resetAll();
</script>
</body>
</html>
